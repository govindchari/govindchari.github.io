I"Y<h2 id="introduction">Introduction</h2>
<p>Lets say you flip a coin ten times and it comes up heads six out of ten times. Would you think this coin is biased? Probably not.</p>

<p>Now lets say you flip a coin a thousand times and it comes up heads six hundred times. You would probably think it is biased. But why?</p>

<p>In both cases the coin comes up heads 60% of the time. How can we quantify our intuition here? We will turn to the world of hypothesis testing to answer this question.</p>

<h2 id="null-and-alternative-hypothesis">Null and Alternative Hypothesis</h2>

<p>The null hypothesis is the statement we are trying develop evidence against and the alternative hypothesis is its complement.</p>

<p>Here our null hypothesis is that the coin is fair. The alternative hypothesis is that the coin is biased.</p>

<h2 id="p-value">P-Value</h2>
<p>One useful question for us to ask is what is the chance that we see an event this or more extreme due to random chance assuming the coin where unbiased?</p>

<p>The statistical term for this is p-value.</p>

<p>If our p-value is small it means that it is unlikely that we see six hundred heads out of one thousand flips coming up heads if the coin were fair. So a smaller p-value would make us conclude that the coin is indeed biased.</p>

<h2 id="level-of-significance">Level of Significance</h2>
<p>But how small of a p-value is small enough for us to conclude the coin is biased? In statistics, this value is called $\alpha$ and a typical value for $\alpha$ is 0.05. But what does this $\alpha$ really mean?</p>

<p>It is the chance that we conclude that the coin is biased when in reality it isn’t. This kind of error is called type I error.</p>

<p>So if $p &lt; 0.05$ then we will conclude that the coin is biased, and we know there is a 5% chance that we conclude the coin is biased when in reality it is not.</p>

<h2 id="example">Example</h2>
<p>Let’s dig into the math of computing p-values. By definition, the p-value is the chance that we see six hundred or more heads out of a thousand flips assuming the coin where unbiased.</p>

<p>Let $X$ be a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a> random variable. If $X=0$ then the coin landed on tails, and if $X=1$ the coin landed on heads. Since we are assuming the coin is unbiased we can write the probability mass function, expected value, and variance of $X$ as follows</p>

\[\mathbb{P}[X=0] = 0.5\]

\[\mathbb{P}[X=1] = 0.5\]

\[\mathbb{E}[X] = 0.5\]

\[\mathbb{V}[X] = 0.25\]

<p>The number of heads that we get is the sum of X from 1 to a thousand. We will define this new random variable as $H$. We can write its expected value and variance as follows</p>

\[\mathbb{E}[H] = 500\]

\[\mathbb{V}[H] = 250\]

<p>Since H is the sum of a large number of independent and identically distrubuted random variables, we can use the <a href="https://www.youtube.com/watch?v=zeJD6dqJ5lo&amp;ab_channel=3Blue1Brown">central limit theorem</a> to conclude that in addition to having the expected value and variance written above we know that $H$ is normally distributed. We can then calculate the <a href="https://en.wikipedia.org/wiki/Standard_score">z-score</a> of flipping 6 million heads as follows:</p>

\[z = \frac{x-\mu}{\sigma} = \frac{600 - \mathbb{E}[H]}{\sqrt{\mathbb{V}[H]}} \approx 6.32\]

<p>To get the p-value from the z-score we use the standard gaussian cumulative density function as follows:</p>

\[p = 2(1 - \phi(z)) \approx 2 \times 10^{-10}\]

<p>This p-value is less than 0.05 so we can conclude that the coin is biased.</p>

<h2 id="code">Code</h2>
<p>Below is some Julia code that conducts a hypothesis test for a coin flip example. The input parameters are the number of heads and the number of coin flips. The source file can be found <a href="https://github.com/govindchari/blog-code/blob/main/statistics/hypothesis_testing.jl">here</a>. I encourage you to play around with the number of heads and total number of flips to get an intuition for what is statistically significant and what isn’t.</p>

<figure class="highlight"><pre><code class="language-julia" data-lang="julia"><span class="k">using</span> <span class="n">Distributions</span>

<span class="c"># Input Parameters</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">num_flips</span> <span class="o">=</span> <span class="mi">100</span>
<span class="nd">@assert</span> <span class="n">num_heads</span> <span class="o">&lt;=</span> <span class="n">num_flips</span> <span class="s">"Number of heads must be less than or equal to the number of flips"</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="c"># Significance level</span>

<span class="c"># Mean and variance of number of heads if the coin were unbiased</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">num_flips</span>
<span class="n">var</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="o">*</span> <span class="n">num_flips</span>

<span class="c"># Z-Score of our observation of num_heads</span>
<span class="n">z</span> <span class="o">=</span> <span class="x">(</span><span class="n">num_heads</span> <span class="o">-</span> <span class="n">mu</span><span class="x">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="x">(</span><span class="n">var</span><span class="x">)</span>

<span class="c"># Compute p-value</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="x">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">cdf</span><span class="x">(</span><span class="n">Normal</span><span class="x">(),</span> <span class="n">abs</span><span class="x">(</span><span class="n">z</span><span class="x">)))</span>

<span class="n">println</span><span class="x">(</span><span class="s">"p-value: "</span><span class="x">,</span> <span class="n">p</span><span class="x">)</span>
<span class="k">if</span> <span class="x">(</span><span class="n">p</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="x">)</span>
    <span class="n">println</span><span class="x">(</span><span class="s">"p &lt; 0.05 so we can reject the null hypothesis and conclude the coin is biased"</span><span class="x">)</span>
<span class="k">else</span>
    <span class="n">println</span><span class="x">(</span><span class="s">"p &gt; 0.05 so we cannot reject the null hypothesis and we cannot conclude that the coin is biased"</span><span class="x">)</span>
<span class="k">end</span></code></pre></figure>

<hr />
:ET